INTRODUCTION
============

This pipeline is to recover orthologous exons and their flanking sequences from target enrichment data

Pipeline can be divided into 3 steps: 

(1) Data preparation
(2) Assembling
(3) Further processing

This documentation includes a step-by-step tutorial using a small test dataset. It will
help you to familiarize with this pipeline. 


DOCUMENTATION
=============

A manual page containing a description of all options can be accessed by option "-h" or 
"--help". For example, full options of "assemble.pl" can be accessed by:

	$ assemble.pl -h
		
	or
		
	$ assemble.pl --help


SYSTEM REQUIREMENTS
===================

(1) Data preparation:

Softwares: (Please put them under $PATH)

	Perl v5.18 or higher
	trim_galore v0.4.1 or higher
	cutadapt v1.2.1 or higher
	BioPython (optional)

Perl module:
	
	Bio::Seq (Included in Bioperl)
    Parallel::Forkmanager
	
(2) Assembling:

Softwares: (Please put them under $PATH)

	Perl v5.18 or higher
	USEARCH 10.0.240 or higher
	SGA v0.10.15 or higher
	Exonerate v2.2.0 or higher
	
Perl module: 

	Bio::Seq (Included in Bioperl)
    Parallel::Forkmanager
    Sys::Info

NOTE: system requirements for assembling can be checked by option "--check_depends", usage
of it will be mentioned later

(3) Further processing:

This step is optional. So system requirements for this step are not list here. Please check
requirements for these scripts by "-h" or "--help" options


SCRIPTS FOR EACH STEP
=====================
All scripts were installed at ~/local/pipeline_scripts and all of them have been placed under
$PATH

(1) Data preparation:
	
	gunzip_Files.pl (Expand gunzipped raw data)
	trim_adaptor.pl (Trim low quality bases and adaptors)
	demultiplex_inline.pl (Demultiplex samples which inline index are involved)
	query_translate.pl (Generate coding sequences for reference and tranlate into AA)
	predictFrames (Predict coding frame of reference)
	ensmbl2frames.py (Module included in predictFrames)
	
(2) Assembling:
	
Main script:
		
	assemble.pl (Wrapper around several scripts to recover orthologous exons and their flanking sequences from target enrichment data)
		
Called scripts:
	
	rmdup.pl (Remove PCR duplication)
	ubxandp.pl (Parse reads to loci)
	sga_assemble.pl (Assemble parsed reads)
	exonerate_best.pl (Filter unqualified assemblies and find assemblies can be further assembled)
	merge.pl (Assemble contigs further and retrieve best contigs for each locus)
	reblast.pl (Remove potential paralogs)

(3) Further processing:
	
	Manipulate dataset:
	pick_taxa.pl (Pick out needed taxa or discard unneeded taxa)
	merge_loci.pl (Merge sequences under several directories from the same loci)
	get_orthologues.pl (Find sequences orthology to reference from existing genomes)
	
	Align:
	mafft_aln.pl (Align nucleotide sequences in codon or normally)
	
	Filter:
	filter.pl (Remove badly aligned sequences)
	flank_filter.pl (Discard too variable flanking regions)
	clocklikeness_test.pl (Pick out loci which follows molecular clock hypothesis)
	monophyly_test.pl (Pick out loci which topology is not congurence with provided monophyly group)
	detect_contamination.pl (Detect contaminated samples)
	
	Statistics:
	statistics.pl (Summary statistics for each locus and sample)
	map_statistics.pl (Summary statistics for each sample from duplication-marked bam file)
	count_reads_bases.pl (Count number of base pairs and reads for fastq file)

	SNP-based analysis
	consensus.pl (Make majority consensus sequences)
	gatk.sh (Wrapper to call SNPs by GATK)
	vcftosnps.pl (Convert output from GATK into other format)
	
	Phylogenetic analysis
	concat_loci.pl (Concatenate all loci into a master gene)
	construct_tree.pl (Construct constrained or not constrained ML trees in batch)

	Others:
	unixlb_unwarp.pl (Substitute line breaks and unwrap sequences of fasta file)

NOTE: Only part of scripts will be demonstrated in following tutorial. Please use "-h" or "--help" to
see the detailed usage and options for each script 


INSTALLATION
============
All softwares and scripts have been placed under $PATH in Oceanus


TUTORIAL
========
The purpose of this tutorial is to help familiarize you with the format of the input you
need and output you should expect from running this pipeline. The tutorial uses a test
dataset (in "tutorial.tar.gz") that is a subset of real Illumina data from an enriched library.

This tutorial assumes that you have some experience executing programs from the command line
on a UNIX-like system. If you would like to learn more about the command line, or need a refresher,
you can find a command line tutorial in "introToCmdLine.pdf".


TEST DATA
=========
Test data is under "toturial.tar.gz".

First, let's expand "toturial.tar.gz":

	$ tar -zxvf toturial.tar.gz toturial_test_data

Change directory to tutorial:

	$ cd test_data

Under "tutorial" there are:
(1) raw_reads: A folder containing 2 gzipped raw reads. Structure of folder looks like:

	raw_reads
	    └── test1
	    |	 ├── test1_R1.fq.gz
		|	 └── test1_R2.fq.gz
		└── test2
		     ├── test2_R1.fq.gz
			 └── test2_R2.fq.gz

(2) inlineindex.txt: File records the sequences and number of inline index. There's 5 columns
in the file, each column is delimited by tab. Its format looks like:

	Index		Name	Sequence		Name	Sequence
	TCTGCC		IS1_Ind1	A*C*A*C*TCTTTCCCTACACGACGCTCTTCCGATCTtc*t*g*c*c		IS3_Ind1	ggcagaAGATCGGAA*G*A*G*C
	GTCTCT		IS1_Ind2	A*C*A*C*TCTTTCCCTACACGACGCTCTTCCGATCTgt*c*t*c*t		IS3_Ind2	agagacAGATCGGAA*G*A*G*C	

Each column is:
Column 1 is the sequence of 6 bp inline index.
Column 2 is the name of P5 inline index. The last character of name MUST BE NUMBER, and
MUST BE same as the number in Column 4.
Column 3 is the sequence of P5 inline index.
Column 4 is the name of P7 inline index. The last character of name MUST BE NUMBER, and
MUST BE same as the number in Column 2.
Column 5 is the name of P7 inline index.
	
(3) indexpair.txt: File records pairs of inline index for each sample. There's 4 columns
in the file, each column is delimited by tab. Its format looks like:

	yang2-01	CL452_1	1	25
		CL452_2	2	26
	yang2-02	CL1238_2	1	26
		CL1238_3	2	27

Each column is:
Column 1 is the name of undemultiplexed sample.
Column 2 is the name of sample comprised in undemultiplexed sample.
Column 3 is the number of P5 inline index of comprised sample.
Column 4 is the number of P7 inline index of comprised sample.

In this example, there are two undemultiplexed sample (yang2-01 and yang2-02). Each undemultiplexed
sample comprises 2 samples. "yang2-01" has "CL452_1" and "CL452_2". "yang2-02" has "CL1238_2"
and "CL1238_3". The P5 inline index of CL452_1 is "1". The P7 inline index of CL452_1 is "25". 
The rest can be deduced in same manner.

If inline index is not added, the number of inline index is "na", like:

	yang2-03	1263_1	na	na
	
(4) Oreochromis_niloticus.frames.fas: DNA sequences of reference in fasta format. Header
of each sequence looks like:

	>Danio_rerio.1.10018393.10018273.1

	or

	>Danio_rerio.1.15377112.15377347.2

	or

	>Danio_rerio.1.27819464.27819310.3

">xxx.1" means this sequence should be translated from first nucleotide to get an appropriate
peptide. The redundant nucleotides haven't excluded which are not involved in DNA->AA
translation. This file can be generated from predictFrames. Please refer to 
~/local/pipeline_scripts/data_preparation/predictFrames.README for more detail.

(5) Oreochromis_niloticus.genome.fas: Soft-masked genomic DNA sequences in fasta format.
(It's not a real genome, but we treat it as a genome in this tutorial for convenience).
In real case, well soft-masked genomic sequences can be downloaded from Ensembl. 

(6) species1.genome.fas and species2.genome.fas: 2 genome sequences of other species


PRELIMINARY REMARKS
===================

(1) Throughout this tutorial, the following marks a shell command:

	$ some_command

This means you should enter some_command at the shell prompt. The `$` sigil is
not part of the command.

(2) Every files under analysis must be encoded by unicode (utf-8) and line breaks is unix (LF).

(3) Encoding, line break can be checked by text editor (like TextWrangler or BBEdit) when you save
your file. Data from non-Unix-like system (like windows) are likely to have different encoding and
line breaks, so they must be checked before analysis.

(4) Line breaks and sequences of fasta files can be substituted then unwrapped by unixlb_unwrap.pl
 
(5) Avoid using non-English charaters, space or any other strange characters like "#?@!" in filename

(6) All scripts and softwares on Oceanus have been placed under $PATH, so interpreter and path
to scripts are NOT required when running the scripts

DATA PREPARATION
================

## Step 1: Gunzip data
First, let's expand gunzipped raw data:

	$ gunzip_Files.pl \
	--gzip raw_reads \
	--gunzipped gunzipped_raw_reads

Involved options:

	--gzip: Directory containing gzipped raw data
	--gunzipped: Directory containing expanded raw data

Output: 
gunzipped_raw_reads: Directory containing expanded raw data

## Step 2: Demultiplex sample based on inline index (optional): 
If inline index are added during the library preperation, samples need to be demultiplexed.

	$ demultiplex_inline.pl \
	--undemultiplexed gunzipped_raw_reads \
	--demultiplexed demultiplexed \
	--inline_index inlineindex.txt \
	--index_pair indexpair.txt
	
Involved options:

	--undemultiplexed: Directory containing expanded raw data
	--demultiplexed: Directory containing demultiplexed raw data
	--inline_index: File records the sequences and number of inline index. Format of input
	file please refer to "inlineindex.txt"
	--index_pair: File records pairs of inline index for each sample. Format of input file
	please refer to "indexpair.txt"

Output: 
demultiplexed: Directory containing demultiplexed raw data
unpaired_reads: Directory containing unpaired reads for each sample

## Step 3: Trim adaptor and low quality bases
Then, trim illumina adaptor and low quality bases.

1) When inline index are involved in samples
	$ trim_adaptor.pl \
	--raw_reads demultiplexed \
	--inline_index inlineindex.txt \
	--index_pair indexpair.txt \
	--trimmed trimmed

Output: 
trimmed: Directory containing reads without adaptor, inline index and low quality bases
trimming_report: Directory containing trimming report for each sample
trimmed_reads_bases_count.txt: Table summarizing number of reads and bases of raw data and 
trimmed data

2) When no inline index are involved in samples
	$ trim_adaptor.pl \
	--raw_reads gunzipped_raw_reads \
	--trimmed trimmed

Output: 
trimmed: Directory containing reads without adaptor and low quality bases
trimming_report: Directory containing trimming report for each sample
trimmed_reads_bases_count.txt: Table summarizing number of reads and bases of raw data and 
trimmed data

Involved options:

	--raw_reads: Directory containing demultiplexed raw data
	--inline_index: File records the sequences and number of inline index. Format of input
	file please refer to "inlineindex.txt" 
	--index_pair: File records pairs of inline index for each sample. Format of input file
	please refer to "indexpair.txt"
	--trimmed: Output directory containing adaptor and low quality bases trimmed reads
		
## Step 4: Query Preparation
First, we need to prepare coding and AA sequences of reference in fasta format.

  	$ query_translate.pl \
	--predicted_frames Oreochromis_niloticus.frames.fas \
	--nucleo_out Oreochromis_niloticus.dna.fas \
	--aa_out Oreochromis_niloticus.aa.fas

Involved options:

	--predicted_frames: DNA Sequences of reference with redundant nucleotides
	--nucleo_out: Coding DNA sequences of reference
	--aa_out: AA sequences of reference

Output: 
Oreochromis_niloticus.dna.fas: Coding DNA sequences of reference
Oreochromis_niloticus.aa.fas: Amino acid sequences of reference


ASSEMBLE
========

All inputs for assembly has been prepared. Let's start assembling now. The main script
and called another 6 scripts were placed under $PATH.

6 scripts represent 6 steps of assembly. They are called by main script in following procedure:
1) rmdup.pl: Remove PCR duplicates
2) ubxandp.pl: Parse reads to targeted loci
3) sga_assemble.pl: Assemble reads for each locus
4) exonerate_best.pl: Filter unqualified contigs and find contigs which might be furtherly assembled  
5) merge.pl: Assemble contigs further and retrieve best contigs
for each locus
6)  reblast.pl: Remove potential paralogs

(1) Run whole pipeline
----------------------

Normally, we run the whole pipeline (cleaned reads in, orthologous assemblies out), which includes
3 steps:

## Step 1: Check requirements of assembling
Before running the script, we need to check requirements which can be checked by "--check_depends".

	$ assemble.pl \
	--check_depends 
	
Involved options:

	--check_depends: Check all dependencies for assemble.pl

If all dependencies are properly installed, you will see the following text in STDOUT:

Currently used interpreter is "/XXX/perl"

Version of your perl interpreter (/XXX/perl) is v5.24

All modules are properly installed

All softwares are properly installed

All scripts are found under $PATH


## Step 2: Check the existence of reference sequences in given genome:
Determination of orthology between reference and enriched sequence is based on whether they can be
aligned to same position on the genome of reference species. Thus, existence of reference must be 
checked to avoid false negative detection of orthology due to missing targeted loci.

	$ assemble.pl \
	--check_query \
	--queryn Oreochromis_niloticus.dna.fas \
	--db Oreochromis_niloticus.genome.fas \
	--dbtype nucleo
	
Involved options:

	--check_query: Check whether reference sequences existing in given database, and return
	list of missing loci, then exit
	--queryn: DNA sequences of reference in fasta format 
	--db: Path to DNA or AA database, either in Fasta or UDB format
	--dbtype: Database type either 'nucleo' for DNA or 'prot' for AA database

If input genome is in Fasta format, a corresponding UDB database will be generated. If input
database is in udb format, then no file will be generated. You will see the following text
in STDOUT in both case:

	Start constructing ublast database in parallel
	Something generated by usearch...
	Ublast database has been constructed
	Something generated by usearch...
	#### All genes are found in provided database ####

If some genes do not exist in given genome, STDOUT will be:

	#### 2 genes below are not found in provided database  ####
	Danio_rerio.1.46410167.46410317
	Danio_rerio.14.21871634.21871111

## Step 3: Assemble:
Requirements and existence of target loci in given genome have been checked. Let's start assemble.

	$ assemble.pl \
	--trimmed trimmed \
	--queryp Oreochromis_niloticus.aa.fas \
	--queryn Oreochromis_niloticus.dna.fas \
	--db Oreochromis_niloticus.genome.fas \
	--dbtype nucleo \
	--ref_name Oreochromis_niloticus \
	--outdir assemble_result

Involved options:

	--trimmed: Directory containing reads without adaptor and low quality bases
	--queryp: Amino acid sequences of target loci in fasta format 
	--queryn: Nucleotide sequences of target loci in fasta format 
	--db: Path to DNA or amino acid database, either in fasta or udb format
	--dbtype: Database type either 'nucleo' for DNA or 'prot' for amino acid database
	--ref_name: Substitute name of target loci as --ref_name in the output of last step
	(reblast.pl), disabled in default
	--outdir: Directory to pipeline output

Several folders and files will be generated during the execution:
1) run_dir: All intermediate outputs will be generated under this folder.
2) samplelist.txt: A list includes name of all samples
3) rmdup_reads_bases_count.txt: A table records number of reads and bases before and after
removing PCR duplicates
4) enriched_loci.txt: A table records number of total loci, number of enriched loci and
percentage of enriched loci for each sample
5) Oreochromis_niloticus.genome.fas.udb: UDB of "Oreochromis_niloticus.genome.fas". This 
can be used as input database.

Output will be placed under "assemble_result" including 3 folders:
1) nf: folder containing coding nucleotide sequences
2) f: folder containing coding sequences with flanking regions
3) p: folder containing AA sequences

(2) Clean intermediate output
------------------------

Intermediate output under "run_dir" would occupy lot of memory. Remove "run_dir" and all
files under it by:

	$ assemble.pl \
	--clean


(3) Run partial pipeline
------------------------

If something goes wrong at the intermediate step, don't worry, assemble.pl is able to restart
from intermediate step. It can also stop at the step you want.

To restart from intermediate step, 4 things are essentially needed:
1) Output from previous one step
2) Essential options for the following step
3) Options for restart or stop
4) sample list

1) Intermediate output from each step
Step 1: rmdup.pl: ./run_dir/rmdup
Step 2: ubxandp.pl: ./run_dir/parsed
Step 3: sga_assemble.pl: ./run_dir/assembled
Step 4: exonerate_best.pl: ./run_dir/filtered
Step 5: merge.pl: ./run_dir/merged
Step 6: reblast.pl: ./run_dir/reblastout

2) Essential options for each step
Step 1: rmdup.pl: --trimmed
Step 2: ubxandp.pl: --queryp
Step 3: sga_assemble.pl: nothing
Step 4: exonerate_best.pl: --queryp
Step 5: merge.pl: --queryp and --queryn
Step 6: reblast.pl: --db, --dbtype, --ref_name and --outdir

3) Option for restart or stop at a step
To restart from a step: --restart_from_xxx
To stop at a step: --stop_after_xxx

For example, I want restart from step 4 (exonerate_best.pl). The option is:

	--restart_from_exonerate_best

I want stop at step 5 (merge.pl). The option is:

	--stop_after_merge

I want restart from step 4 and stop at step 5, then specify 2 options:

	--restart_from_exonerate_best --stop_after_merge

4) Sample list
Sample list is named as "samplelist.txt" in default. It contains the list of sample names,
one sample name per line. It is automatically generated from first step. It looks like:
test1
test2

Here are 3 examples of running partial pipeline:
EXAMPLE 1: From an intermediate step to the end (exonerate_best.pl -> end)
We need:
1) Output from previous one step "sga_assemble.pl" (./run_dir/assembled)
2) Essential inputs of exonerate_best.pl (--queryp), merge.pl (--queryn, --queryp) and 
reblast.pl (--db, --dbtype, --ref_name, --outdir)
3) samplelist.txt
4) option "--restart_from_exonerate_best"
so the command is:

	$ assemble.pl \
	--queryp Oreochromis_niloticus.aa.fas \
	--queryn Oreochromis_niloticus.dna.fas \
	--db Oreochromis_niloticus.genome.fas \
	--dbtype nucleo \
	--outdir assemble_result \
	--ref_name Oreochromis_niloticus \
	--samplelist samplelist.txt \
	--restart_from_exonerate_best

EXAMPLE 2: Restart from an intermediate step to another intermediate step (sga_assemble.pl -> merge.pl)
We need:
1) output from previous one step ubxandp.pl (./run_dir/parsed)
2) Essential inputs of sga_assemble.pl (nothing), exonerate_best.pl (--queryp) and merge.pl
(--queryn, --queryp)
3) samplelist.txt
4) 2 options "--restart_from_sga_assemble" as well as "--stop_after_merge"
command:

	$ assemble.pl \
	--queryp Oreochromis_niloticus.aa.fas \
	--queryn Oreochromis_niloticus.dna.fas \
	--samplelist samplelist.txt \
	--restart_from_sga_assemble \
	--stop_after_merge

EXAMPLE 3: Stop at an intermediate step (start -> sga_assemble.pl)
We start from the first step, so there's no input from previous one step. We just need:
1) Essential inputs of rmdup.pl(--trimmed), ubxandp.pl(--queryp), sga_assemble.pl(nothing),
2) samplelist.txt
3) option "--stop_after_sga_assemble"
command:

	$ assemble.pl \
	--trimmed trimmed \
	--queryp Oreochromis_niloticus.aa.fas \
	--samplelist samplelist.txt \
	--stop_after_sga_assemble

(4) Only assemble part of samples
---------------------------------

Samples exist in samplelist.txt will be assembled. You can only write name of samples which
need to be assembled. For example, I want to assemble test1 only, then the list is:
test1

Then, specify the option "--samplelist" to input your list:

	$ assemble.pl \
	--trimmed trimmed \
	--queryp Oreochromis_niloticus.aa.fas \
	--queryn Oreochromis_niloticus.dna.fas \
	--db Oreochromis_niloticus.genome.fas \
	--dbtype nucleo \
	--outdir assemble_result \
	--ref_name Oreochromis_niloticus \
	--samplelist samplelist.txt


FURTHER PROCESSING
===========================

Before downstream analysis, datasets probably need to be modified. Sequences are required 
to be aligned, and poorly aligned sequences should be discarded. We also need to access
statistics of filtered alignments. So further processing mainly
includes:
1) Manipulate dataset
2) Aligning
3) Filtering
4) Summary statistics

(1) Manipulate dataset (optional):
----------------------------------------------------------

Before aligning and filtering, Some users may want to add orthologue sequences from existing
genomes or delete poorly enriched sequences. Before introducing how to do it, we emphasize
once again that:

	SEQUENCES MUST BE ADDED OR DELETED BEFORE ALIGNING!!!

1) Add orthologous sequences 

Dependencies:
USEARCH v10.0.240 or higher
BioPerl v1.007001 or higher

## Step 1: Extract orthologous sequences from existing genomes
First, we extract sequences orthology to loci in "Oreochromis_niloticus.dna.fas" from
"species1.genome.fas" and "species2.genome.fas"

	$ get_orthologues.pl \
	--query Oreochromis_niloticus.dna.fas \
	--querydb Oreochromis_niloticus.genome.fas \
	--subdb "species1.genome.fas|species2.genome.fas" \
	--subname "species1 species2" \
	--outdir orthologs \
	--cpu 12

Involved options:

	--query
	  Coding DNA sequences of reference
	--querydb
	  Space delimited list of one or more DNA databases of reference species in either in 
	  FASTA or UDB format
	--subdb
      List of DNA databases of subjects in FASTA or UDB format ONLY, but database format 
      of the same subject need to consistent. Input database list of different subjects 
      are delimited by '|', and database belonging to the same subject are delimited by 
      space. e.g. \"sp1.genome.1.fas sp1.genome.2.fas|sp2.genome.1.udb sp2.genome.2.udb\"
	--subname
	  Space delimited list of subject name in output, which is one-to-one match to the 
	  list of subject databases. 
	--outdir
	  Name of output directory, which has 2 subfolders including 'nf' for coding sequences
	  and 'p' for AA sequences.
	--cpu
      Limit the number of CPUs, 1 in default
      
Output:
orthologs: Directory includes sequences orthology to reference
Oreochromis_niloticus.genome.fas.udb: udb of "Oreochromis_niloticus.genome.fas".
species1.genome.fas.udb: udb of "species1.genome.fas".
species2.genome.fas.udb: udb of "species2.genome.fas".


## Step 2: Add them into datasets
Then, we add coding sequences from "species1.genome.fas" and "species2.genome.fas" to
enriched datasets. Each locus should have at least 3 sequences.

	$ merge_loci.pl \
	--indir "assemble_result/nf orthologs/nf" \
	--outdir merged_nf \
	--min_seq 3
	
Involved options:

	--indir
	  List of dir containing sequences
	--outdir
	  Dir containing merged loci files
	--min_seq
	  Minimum sequences required in merged file, 2 in default

Output:
merged_nf: Dir containing merged loci files

2) Delete unneeded sequences

Dependencies: 
Nothing

1. Discard taxa ("species2") by "--deselected_taxa"
	$ pick_taxa.pl \
	--indir merged_nf \
	--outdir merged_nf_deselected \
	--deselected_taxa "species2"

Output:
merged_nf_deselected: Dir containing sequences of without discarded taxon

Involved options:

	--indir
	  Dir containing unaligned sequences
	--outdir
	  Dir containing sequences of selected taxon
	--deselected_taxa
	  List of taxa want to be discarded, each taxon is delimited by space

(2) Aligning
------------
Then, let's align each loci.	

Dependencies:
(1) BioPerl v1.007001 or higher
(2) Mafft v7.294b or higher (rename it as "mafft" and put it under \$PATH)

1) If input sequences are full-coding sequence:

	$ mafft_aln.pl \
	--dna_unaligned merged_nf \
	--dna_aligned merged_nf_aligned \
	--cpu 12
	
Output:
merged_nf_aligned: Dir containing nucleotide sequences aligned in codon

2) If input sequences are not coding sequence or coding sequences with flanks;

	$ mafft_aln.pl \
	--dna_unaligned assemble_result/f \
	--dna_aligned f_aligned \
	--non_codon_aln \
	--cpu 12
	
Output:
f_aligned: Dir containing aligned nucleotide sequences

Involved options:

	--dna_unaligned
	  Dir containing unaligned DNA sequences
	--dna_aligned
	  Dir containing aligned DNA sequences, named as "xx_aligned" if this option is not specified
	--non_codon_aln
	  Do not align DNA sequences in codon. This option is turned off by default
	--cpu
	  Limit the number of CPUs, 1 in default.

(3) Filtering
-------------
1) After aligning, we can filter poorly aligned sequences:

Dependencies:
(1) BioPerl v1.007001 or higher
(2) Mafft v7.294b or higher (rename it as "mafft" and put it under \$PATH)

Coding sequences are input. Only remove poorly aligned sequences, and "Oreochromis_niloticus" is reference: 

	$ filter.pl \
	--indir merged_nf_aligned \
	--filtered merged_nf_filtered \
	--ref_taxa "Oreochromis_niloticus" \
	--cpu 12

Output:
merged_nf_filtered: Dir containing filtered alignments which are aligned in codon

Involved options:

	--indir:
	  Dir containing unfiltered alignments
	--filtered:
	  Dir containing filtered alignments
	--ref_taxa:
	  A space delimit list of reference taxa
    --cpu
      Limit the number of CPUs, 1 in default
      
NOTE: Remember to check filtered alignments. Please tuning the parameters, if too much
alignments are filtered. Use "-h" or "--help" to access detailed parameter

2) To explore abundant variabilities in flanking regions, we need to discard too variable
flanking sequences first:

Dependencies:
(1) BioPerl v1.007001 or higher
(2) Mafft v7.294b or higher (rename it as "mafft" and put it under \$PATH)

Filter sequences with flanking regions in 'f_aligned', and only files and sequences co-exist
in 'f_aligned' and 'merged_nf_aligned' will be written to 'f_filtered'. Sequence of "Oreochromis_niloticus"
is reference. Run script with 4 process: 
	
	$ flank_filter.pl \
	--flank f_aligned \
	--nonflank_filtered merged_nf_filtered \
	--flank_filtered f_filtered \
	--ref_taxa "Oreochromis_niloticus" \
	--cpu 4

Output:
f_filtered: Dir containing well-aligned coding sequences with flanks

Involved options:

	--flank:
	  Dir containing unfiltered alignments with flanks
	--nonflank_filtered:
	  Dir containing well-aligned coding sequences
	--flank_filtered:
	  Dir containing well-aligned coding sequences with flanks
	--ref_taxa:
	  A space delimit list of reference taxa
    --cpu
      Limit the number of CPUs, 1 in default

3) Besides filtering poorly aligned sequences, we additionally are provided several
kinds of filter for different purposes:

1. Users can pick out loci which topologies are not congurence with provided monophyletic
group by monophyly_test.pl. This analysis needs unconstrained ML tree and ML tree constrained
by provided monophyletic group for each locus, which can be generated by construct_tree.pl.

NOTE: Format of input file after "--mono_constrain" (option in monophyly_test.pl) and
"--constrain" (option in monophyly_test.pl) can be found at ~/local/pipeline_scripts/postprocess/monogroup.txt

2. Some analysis need genes follow the molecular clock hypotheses (like construction of
time-recalibrated tree). Users can filter loci which disobey the molecular clock hypotheses
by clocklikeness_test.pl

3. Contamination may confound the results of phylogenomic analysis. Potential contaminated
samples can be detected as well by detect_contamination.pl. 

NOTE: Format of input file after "--same" (option in detect_contamination.pl) and can be
found at ~/local/pipeline_scripts/postprocess/same.txt


(4) Summary statistics
----------------------
Finally, let's summary statistics of filter alignment.

Dependencies:
Perl module:
Bio::AlignIO (included in Bioperl)
Bio::Align::DNAStatistics (included in Bioperl)

Summary statistics from coding sequences with and without flanks.

Summarized statistcis for each locus including:
(1) Average length of coding region
(2) Average length of flanking region
(3) Length of alignment
(4) Average GC content
(5) Percentage of Missing data
(6) Pairwise distance
Summarized statistcis for each sample including:
(1) Average length of captured sequences
(2) Average GC content
(3) Number of captured loci 

	$ statistics.pl \
	--nf_aligned merged_nf_filtered \
	--f_unaligned assemble_result/f
	
Involved options: 
--nf_aligned: 
  Folder comprising aligned full-coding sequences
--f_unaligned:
  Folder comprising unaligned whole sequences (include flanking sequences)

Output:
1) loci_summary.txt: Tab delimited table of summary statistics for each locus
2) sample_summary.txt: Tab delimited table of summary statistics for each sample

(4) Phylogenetic analysis
-------------------------

Several scripts were provided to reformat filtered alignments as input of phylogenetic analysis.
Alignment files can be rearranged and input for analysis in following procedure:

Concatenated tree:
Filter alignments -> concat_loci.pl (concatenate loci into master gene) -> RAxML

Species tree:

Filter alignments -> construct_tree.pl (construct gene tree for each loci) -> merge resulting gene trees into one file -> ASTRAL

SNP-based analysis:

Filter alignments -> consensus.pl (generate majority consensus reference) --
                                                                           |-> gatk.sh (call SNPs from reads) -> vcftosnps.pl (reformat vcf output of gatk.sh) -> BEAST, STRUCTURE, dudi.pca
                                                            Trimmed reads --


